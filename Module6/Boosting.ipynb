{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Boosting and the AdaBoost Method\n\nUsing **ensemble methods** can greatly improve the results achieved with weak machine learning algorithms, also called **weak learners**. Ensemble methods achieve better performance by aggregating the results of many statistically independent models. This process averages out the errors and produces a better, final, prediction. \n\nIn this lab you will work with widely used ensemble method known as **boosting**. Boosting is a meta-algorithm since the method can be applied to many types of machine learning algorithms. In summary, boosting iteratively improves the learning of the N models by giving greater weight to training cases with larger errors. The basic boosting procedure is simple and follows these steps:\n1. N learners (machine learning models) are defined.\n2. Each of i training data cases is given an initial equal weight of 1/i.\n3. The N learners are trained on the weighted training data cases.\n4. The prediction is computed based on a aggregation of the learners; averaging over the hypothesis of the N learners. \n5. Weights for the training data cases are updated based on the aggregated errors made by the learners. Cases with larger errors are given larger weights. \n6. Steps 3, 4, and 5 are repeated until a convergence criteria is met.\n\n**Classification and regression tree models** are the weak learners most commonly used with boosting. In this lab you will work with one of the most widely used and successful boosted methods, known as **AdaBoost** or **adaptive boosting**. AdaBoost uses some large number, N, tree models. The rate at which weights are updated is **adaptive** with the errors. \n\nIt is important to keep in mind that boosted machine learning is not robust to significant noise or outliers in the training data. The reweighting process gives greater weight to the large errors, and therefore can give undue weight to outliers and errors. In cases where data is noisy, the random forest algorithm may prove to be more robust. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Example: Iris dataset\n\nAs a first example you will use AdaBoost to classify the species of iris flowers. \n\nAs a first step, execute the code in the cell below to load the required packages to run the rest of this notebook. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import preprocessing\nfrom sklearn import datasets ## Get dataset from sklearn\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport numpy.random as nr\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "To get a feel for these data, you will now load and plot them. The code in the cell below does the following:\n\n1. Loads the iris data as a Pandas data frame. \n2. Adds column names to the data frame.\n3. Displays all 4 possible scatter plot views of the data. \n\nExecute this code and examine the results. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def plot_iris(iris):\n    '''Function to plot iris data by type'''\n    setosa = iris[iris['Species'] == 'setosa']\n    versicolor = iris[iris['Species'] == 'versicolor']\n    virginica = iris[iris['Species'] == 'virginica']\n    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n    x_ax = ['Sepal_Length', 'Sepal_Width']\n    y_ax = ['Petal_Length', 'Petal_Width']\n    for i in range(2):\n        for j in range(2):\n            ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = 'x')\n            ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = 'o')\n            ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = '+')\n            ax[i,j].set_xlabel(x_ax[i])\n            ax[i,j].set_ylabel(y_ax[j])\n            \n## Import the dataset from sklearn.datasets\niris = datasets.load_iris()\n\n## Create a data frame from the dictionary\nspecies = [iris.target_names[x] for x in iris.target]\niris = pd.DataFrame(iris['data'], columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\niris['Species'] = species\n\n## Plot views of the iris data            \nplot_iris(iris) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can see that Setosa (in blue) is well separated from the other two categories. The Versicolor (in orange) and the Virginica (in green) show considerable overlap. The question is how well our classifier will separate these categories. \n\nScikit Learn classifiers require numerically coded numpy arrays for the features and as a label. The code in the cell below does the following processing:\n1. Creates a numpy array of the features.\n2. Numerically codes the label using a dictionary lookup, and converts it to a numpy array. \n\nExecute this code."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "Features = np.array(iris[['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']])\n\nlevels = {'setosa':0, 'versicolor':1, 'virginica':2}\nLabels =  np.array([levels[x] for x in iris['Species']])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, execute the code in the cell below to split the dataset into test and training set. Notice that unusually, 100 of the 150 cases are being used as the test dataset. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 100)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As is always the case with machine learning, numeric features  must be scaled. The code in the cell below performs the following processing:\n\n1. A Zscore scale object is defined using the `StandarScaler` function from the Scikit Learn preprocessing package. \n2. The scaler is fit to the training features. Subsequently, this scaler is used to apply the same scaling to the test data and in production. \n3. The training features are scaled using the `transform` method. \n\nExecute this code."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "scale = preprocessing.StandardScaler()\nscale.fit(X_train)\nX_train = scale.transform(X_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now you will define and fit an AdaBoosted tree model. The code in the cell below defines the model with 100 estimators (trees) using the `AdaBoostClassifer` function from the Scikit Learn ensemble  package, and then fits the model. Execute this code."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(1115)\nab_clf = AdaBoostClassifier()\nab_clf.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the many hyperparameters of the AdaBoosted tree model object are displayed. \n\nNext, the code in the cell below performs the following processing to score the test data subset:\n1. The test features are scaled using the scaler computed for the training features. \n2. The `predict` method is used to compute the scores from the scaled features. \n\nExecute this code. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X_test = scale.transform(X_test)\nscores = ab_clf.predict(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "It is time to evaluate the model results. Keep in mind that the problem has been made deliberately difficult, by having more test cases than training cases. \n\nThe iris data has three species categories. Therefore it is necessary to use evaluation code for a three category problem. The function in the cell below extends code from pervious labs to deal with a three category problem. Execute this code and examine the results."
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def print_metrics_3(labels, scores):\n   \n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score Setosa   Score Versicolor    Score Virginica')\n    print('Actual Setosa      %6d' % conf[0,0] + '            %5d' % conf[0,1] + '             %5d' % conf[0,2])\n    print('Actual Versicolor  %6d' % conf[1,0] + '            %5d' % conf[1,1] + '             %5d' % conf[1,2])\n    print('Actual Vriginica   %6d' % conf[2,0] + '            %5d' % conf[2,1] + '             %5d' % conf[2,2])\n    ## Now compute and display the accuracy and metrics\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    print(' ')\n    print('          Setosa  Versicolor  Virginica')\n    print('Num case   %0.2f' % metrics[3][0] + '     %0.2f' % metrics[3][1] + '      %0.2f' % metrics[3][2])\n    print('Precision   %0.2f' % metrics[0][0] + '      %0.2f' % metrics[0][1] + '       %0.2f' % metrics[0][2])\n    print('Recall      %0.2f' % metrics[1][0] + '      %0.2f' % metrics[1][1] + '       %0.2f' % metrics[1][2])\n    print('F1          %0.2f' % metrics[2][0] + '      %0.2f' % metrics[2][1] + '       %0.2f' % metrics[2][2])\n    \nprint_metrics_3(y_test, scores)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Examine these results. Notice the following:\n1. The confusion matrix has dimension 3X3. You can see that most cases are correctly classified with only a few errors. \n2. The overall accuracy is 0.95. Since the classes are roughly balanced, this metric indicates relatively good performance of the classifier, particularly since it was only trained on 50 cases. \n3. The precision, recall and  F1 for each of the classes is quite good.\n\nTo get a better feel for what the classifier is doing, the code in the cell below displays a set of plots showing correctly (as '+') and incorrectly (as 'o') cases, with the species color-coded. Execute this code and examine the results. "
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "def plot_iris_score(iris, y_test, scores):\n    '''Function to plot iris data by type'''\n    ## Find correctly and incorrectly classified cases\n    true = np.equal(scores, y_test).astype(int)\n    \n    ## Create data frame from the test data\n    iris = pd.DataFrame(iris)\n    levels = {0:'setosa', 1:'versicolor', 2:'virginica'}\n    iris['Species'] = [levels[x] for x in y_test]\n    iris.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Species']\n    \n    ## Set up for the plot\n    fig, ax = plt.subplots(2, 2, figsize=(12,12))\n    markers = ['o', '+']\n    x_ax = ['Sepal_Length', 'Sepal_Width']\n    y_ax = ['Petal_Length', 'Petal_Width']\n    \n    for t in range(2): # loop over correct and incorect classifications\n        setosa = iris[(iris['Species'] == 'setosa') & (true == t)]\n        versicolor = iris[(iris['Species'] == 'versicolor') & (true == t)]\n        virginica = iris[(iris['Species'] == 'virginica') & (true == t)]\n        # loop over all the dimensions\n        for i in range(2):\n            for j in range(2):\n                ax[i,j].scatter(setosa[x_ax[i]], setosa[y_ax[j]], marker = markers[t], color = 'blue')\n                ax[i,j].scatter(versicolor[x_ax[i]], versicolor[y_ax[j]], marker = markers[t], color = 'orange')\n                ax[i,j].scatter(virginica[x_ax[i]], virginica[y_ax[j]], marker = markers[t], color = 'green')\n                ax[i,j].set_xlabel(x_ax[i])\n                ax[i,j].set_ylabel(y_ax[j])\n\nplot_iris_score(X_test, y_test, scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Examine these plots. You can see how the classifier has divided the feature space between the classes. Notice that most of the errors occur in the overlap region between Virginica and Versicolor. This behavior is to be expected.  "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Like most tree-based models, Ada Boosted tree models have a nice property that **feature importance** is computed during model training. Feature importance can be used as a feature selection method. \n\nExecute the code in the cell below to display a plot of the feature importance."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "importance = ab_clf.feature_importances_\nplt.bar(range(4), importance, tick_label = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\nplt.xticks(rotation=90)\nplt.ylabel('Feature importance')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Examine the plot displayed above. Notice that the Sepal_Lenght has virtually no importance. \n\nShould this feature be dropped from the model? To find out, you will create a model with a reduced feature set and compare the results. As a first step, execute the code in the cell below to create training and test datasets using the reduced features."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Create reduced feature set\nFeatures = np.array(iris[['Sepal_Width', 'Petal_Length', 'Petal_Width']])\n\n## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 100)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Next, execute the code in the cell below to define the model, fit the model, score the model and print the results. \n\nOnce you have executed the code, answer **Question 1** on the course page."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(1115)\nab_clf = AdaBoostClassifier()\nab_clf.fit(X_train, y_train)\nscores = ab_clf.predict(X_test)\nprint_metrics_3(y_test, scores) ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These results are identical to those obtained with the full feature set. In all likelihood, there is no significant difference. Given that a simpler model is more likely to generalize, this model is preferred. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Another example\n\nNow, you will try a more complex example using the credit scoring data. You will use the prepared data which had the the following preprocessing:\n1. Cleaning missing values.\n2. Aggregating categories of certain categorical variables. \n3. Encoding categorical variables as binary dummy variables.\n4. Standardizing numeric variables. \n\nExecute the code in the cell below to load the features and labels as numpy arrays for the example. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "Features = np.array(pd.read_csv('Credit_Features.csv'))\nLabels = np.array(pd.read_csv('Credit_Labels.csv'))\nLabels = Labels.reshape(Labels.shape[0],)\nprint(Features.shape)\nprint(Labels.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Nested cross validation is used to estimate the optimal hyperparameters and perform model selection for the AdaBoosted tree model. Since AdaBoosted tree models are efficient to train, 10 fold cross validation is used. Execute the code in the cell below to define inside and outside fold objects. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(123)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(321)\noutside = ms.KFold(n_splits=10, shuffle = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code in the cell below estimates the best hyperparameters using 10 fold cross validation. There are a few points to notice here:\n1. In this case, a grid of one hyperparameter is searched: \n  - learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators\n2. There is a class imbalance and a difference in the cost to the bank of misclassification of a bad credit risk customer. This will be addressed later. \n3. The model is fit on each set of hyperparameters from the grid. \n4. The best estimated hyperparameters are printed. \n\nNotice that the model uses regularization rather than feature selection. The hyperparameter search is intended to optimize the level of regularization. \n\nExecute this code, examine the result, and answer **Question 2** on the course page. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Define the dictionary for the grid search and the model object to search on\nparam_grid = {\"learning_rate\": [0.1, 1, 10]}\n## Define the AdaBoosted tree model\nnr.seed(3456)\nab_clf = AdaBoostClassifier()  \n\n## Perform the grid search over the parameters\nnr.seed(4455)\nab_clf = ms.GridSearchCV(estimator = ab_clf, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)\nab_clf.fit(Features, Labels)\nprint(ab_clf.best_estimator_.learning_rate)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, you will run the code in the cell below to perform the outer cross validation of the model. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(ab_clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\n\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Examine these results. Notice that the standard deviation of the mean of the AUC is more than an order of magnitude smaller than the mean. This indicates that this model is likely to generalize well. \n\nNow, you will build and test a model using the estimated optimal hyperparameters. As a first step, execute the code in the cell below to create training and testing datasets."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code in the cell below defines an AdaBoosted tree model object using the estimated optimal model hyperparameters and then fits the model to the training data. Execute this code."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(1115)\nab_mod = AdaBoostClassifier(learning_rate = ab_clf.best_estimator_.learning_rate) \nab_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "As expected, the hyperparameters of the AdaBoosted tree model object reflect those specified. \n\nThe code in the cell below scores and prints evaluation metrics for the model, using the test data subset. Execute this code and examine the results. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n    \nprobabilities = ab_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Overall, these performance metrics are poor. The majority of negative cases have been misclassified as positive. Adaboosted methods are sensitive to class imbalance. \n\nIt is likely that the poor performance arises from the class imbalance. Notice that there is no way to reweight classes with boosting methods. Some alternatives are:\n1. **Impute** new values using a statistical algorithm. \n2. **Undersample** the majority cases. For this method a number of the cases equal to the minority case are Bernoulli sampled from the majority case. \n3. **Oversample** the minority cases. For this method the number of minority cases are resampled until they equal the number of majority cases.\n\nThe code in the cell below undersamples the majority cases; good credit customers. The `choice` function from the numpy.random package is used to randomize the undersampling. The count of unique label values and the shape of the resulting arrays is printed. Execute this code to create a data set with balanced cases. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "temp_Labels_1 = Labels[Labels == 1]  # Save these\ntemp_Features_1 = Features[Labels == 1,:] # Save these\ntemp_Labels_0 = Labels[Labels == 0]  # Undersample these\ntemp_Features_0 = Features[Labels == 0,:] # Undersample these\n\nindx = nr.choice(temp_Features_0.shape[0], temp_Features_1.shape[0], replace=True)\n\ntemp_Features = np.concatenate((temp_Features_1, temp_Features_0[indx,:]), axis = 0)\ntemp_Labels = np.concatenate((temp_Labels_1, temp_Labels_0[indx,]), axis = 0) \n\nprint(np.bincount(temp_Labels))\nprint(temp_Features.shape)\nprint(temp_Labels.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are now 300 of each label case with 600 cases overall. The question is, will using these data produce better results?\n\nYou will perform model selection and evaluation using nested cross validation. The code in the cell below finds the optimal learning rate parameter using cross validation. \n\nOnce you have executed the code, answer **Question 3** on the course page."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(1234)\ninside = ms.KFold(n_splits=10, shuffle = True)\nnr.seed(3214)\noutside = ms.KFold(n_splits=10, shuffle = True)\n\n## Define the AdaBoosted tree model\nnr.seed(3456)\nab_clf = AdaBoostClassifier()  \n\n## Perform the grid search over the parameters\nnr.seed(4455)\nab_clf = ms.GridSearchCV(estimator = ab_clf, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'roc_auc',\n                      return_train_score = True)\nab_clf.fit(temp_Features, temp_Labels)\nprint(ab_clf.best_estimator_.learning_rate)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice that the estimated optimal learning rate parameter is smaller than before.\n\nNow, run the code in the cell below to execute the outer loop of the cross validation. "
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "nr.seed(498)\ncv_estimate = ms.cross_val_score(ab_clf, Features, Labels, \n                                 cv = outside) # Use the outside folds\n\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate))\nprint('SDT of the metric       = %4.3f' % np.std(cv_estimate))\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate):\n    print('Fold %2d    %4.3f' % (i+1, x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The average AUC is improved compared to the imbalanced training cases. However, the differences are just within 1 standard deviation. Still, there is a reasonable chance the new results represent an improvement. \n\nFinally, you will train and evaluate a model with the balanced cases and the update hyperparameter. The code in the cell below does the following processing:\n1. Creates Bernoulli sampled test and training subsets. \n2. Defines an AdaBoosted model.\n3. Trains the AdaBoosted model.\n\nExecute this code. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Randomly sample cases to create independent training and test data\nnr.seed(1115)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 300)\nX_train = Features[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nX_test = Features[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])\n\n## Undersample the majority case for the training data\ntemp_Labels_1 = y_train[y_train == 1]  # Save these\ntemp_Features_1 = X_train[y_train == 1,:] # Save these\ntemp_Labels_0 = y_train[y_train == 0]  # Undersample these\ntemp_Features_0 = X_train[y_train == 0,:] # Undersample these\n\nindx = nr.choice(temp_Features_0.shape[0], temp_Features_1.shape[0], replace=True)\n\nX_train = np.concatenate((temp_Features_1, temp_Features_0[indx,:]), axis = 0)\ny_train = np.concatenate((temp_Labels_1, temp_Labels_0[indx,]), axis = 0) \n\nprint(np.bincount(y_train))\nprint(X_train.shape)\nprint(y_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "## Define and fit the model\nnr.seed(1115)\nab_mod = AdaBoostClassifier(learning_rate = ab_clf.best_estimator_.learning_rate) \nab_mod.fit(X_train, y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now, execute the code in the cell below to score and evaluate the model.\n\nOnce you have executed the code, answer **Question 4** on the course page."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "probabilities = ab_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "These results are significantly better than those obtained with the imbalanced training data in classifying the negative cases. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Summary\n\nIn this lab you have accomplished the following:\n1. Used an AdaBoosted tree model to classify the cases of the iris data. This model produced quite good results.\n2. Applied feature importance was used for feature selection with the iris data. The model created and evaluated with the reduced feature set had essentially the same performance as the model with more features.  \n3. Used 10 fold to find estimated optimal hyperparameters for an AdaBoosted tree model to classify credit risk cases. The model did not generalize well.\n4. Applied undersampling of the majority cases to create a balanced training dataset and retrained and evaluated the model. The model created with balanced training data was significantly better. "
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}